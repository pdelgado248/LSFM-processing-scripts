{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdN8B91xZO0x"
   },
   "source": [
    "# **1. Install U-Net dependencies**\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiX3Ly-7gA5h"
   },
   "source": [
    "**Load key dependencies**\n",
    "---\n",
    "<font size = 4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11453,
     "status": "ok",
     "timestamp": 1695805629934,
     "user": {
      "displayName": "PABLO DELGADO RODRIGUEZ",
      "userId": "11999848093282927146"
     },
     "user_tz": -120
    },
    "id": "fq21zJVFNASx",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "9d7e5910-32b3-4305-f58f-91b423155d17",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@markdown ##Play to install U-Net dependencies\n",
    "# Install packages which are not included in Google Colab\n",
    "#!pip install data\n",
    "#!pip install -q tifffile # contains tools to operate tiff-files\n",
    "#!pip install -q wget\n",
    "#!pip install -q fpdf2\n",
    "#!pip install -q PTable # Nice tables\n",
    "#!pip install -q zarr\n",
    "#!pip install -q imagecodecs\n",
    "#!pip install \"bioimageio.core>=0.5,<0.6\"\n",
    "\n",
    "#!pip install scikit-image\n",
    "#!pip install matplotlib\n",
    "#!pip install scikit-learn\n",
    "#!pip install ipywidgets\n",
    "\n",
    "from __future__ import print_function\n",
    "Notebook_version = '2.1'\n",
    "Network = 'U-Net (2D) multilabel'\n",
    "\n",
    "\n",
    "from builtins import any as b_any\n",
    "\n",
    "def get_requirements_path():\n",
    "    # Store requirements file in 'contents' directory\n",
    "    current_dir = os.getcwd()\n",
    "    dir_count = current_dir.count('/') - 1\n",
    "    path = '../' * (dir_count) + 'requirements.txt'\n",
    "    return path\n",
    "\n",
    "def filter_files(file_list, filter_list):\n",
    "    filtered_list = []\n",
    "    for fname in file_list:\n",
    "        if b_any(fname.split('==')[0] in s for s in filter_list):\n",
    "            filtered_list.append(fname)\n",
    "    return filtered_list\n",
    "\n",
    "def build_requirements_file(before, after):\n",
    "    path = get_requirements_path()\n",
    "\n",
    "    # Exporting requirements.txt for local run\n",
    "    !pip freeze > $path\n",
    "\n",
    "    # Get minimum requirements file\n",
    "    df = pd.read_csv(path)\n",
    "    mod_list = [m.split('.')[0] for m in after if not m in before]\n",
    "    req_list_temp = df.values.tolist()\n",
    "    req_list = [x[0] for x in req_list_temp]\n",
    "\n",
    "    # Replace with package name and handle cases where import name is different to module name\n",
    "    mod_name_list = [['sklearn', 'scikit-learn'], ['skimage', 'scikit-image']]\n",
    "    mod_replace_list = [[x[1] for x in mod_name_list] if s in [x[0] for x in mod_name_list] else s for s in mod_list]\n",
    "    filtered_list = filter_files(req_list, mod_replace_list)\n",
    "\n",
    "    file=open(path,'w')\n",
    "    for item in filtered_list:\n",
    "        file.writelines(item)\n",
    "\n",
    "    file.close()\n",
    "\n",
    "import sys\n",
    "before = [str(m) for m in sys.modules]\n",
    "\n",
    "#@markdown ##Load key U-Net dependencies\n",
    "\n",
    "#As this notebokk depends mostly on keras which runs a tensorflow backend (which in turn is pre-installed in colab)\n",
    "#only the data library needs to be additionally installed.\n",
    "import tensorflow as tf\n",
    "# print(tensorflow.__version__)\n",
    "# print(\"Tensorflow enabled.\")\n",
    "\n",
    "\n",
    "# Keras imports\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, concatenate, UpSampling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from keras.callbacks import ModelCheckpoint, LearningRateScheduler, CSVLogger # we currently don't use any other callbacks from ModelCheckpoints\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras import backend as keras\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# General import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from skimage import img_as_ubyte, io, transform\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import imread\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "import sys\n",
    "from math import ceil\n",
    "from fpdf import FPDF, HTMLMixin\n",
    "from pip._internal.operations.freeze import freeze\n",
    "import subprocess\n",
    "# Imports for QC\n",
    "from PIL import Image\n",
    "from scipy import signal\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from skimage.util import img_as_uint\n",
    "from skimage.metrics import structural_similarity\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "# For sliders and dropdown menu and progress bar\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.feature_extraction import image\n",
    "from skimage import img_as_ubyte, io, transform\n",
    "from skimage.util.shape import view_as_windows\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Suppressing some warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# BioImage Model Zoo\n",
    "from shutil import rmtree\n",
    "from bioimageio.core.build_spec import build_model, add_weights\n",
    "from bioimageio.core.resource_tests import test_model\n",
    "from bioimageio.core.weight_converter.keras import convert_weights_to_tensorflow_saved_model_bundle\n",
    "from bioimageio.core import load_raw_resource_description, load_resource_description\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "\n",
    "\n",
    "def create_patches(Training_source, Training_target, patch_width, patch_height, min_fraction):\n",
    "  \"\"\"\n",
    "  Function creates patches from the Training_source and Training_target images.\n",
    "  The steps parameter indicates the offset between patches and, if integer, is the same in x and y.\n",
    "  Saves all created patches in two new directories in the /content folder.\n",
    "\n",
    "  Returns: - Two paths to where the patches are now saved\n",
    "  \"\"\"\n",
    "  DEBUG = False\n",
    "\n",
    "  Patch_source = os.path.join('/content','img_patches')\n",
    "  Patch_target = os.path.join('/content','mask_patches')\n",
    "  Patch_rejected = os.path.join('/content','rejected')\n",
    "\n",
    "  #Here we save the patches, in the /content directory as they will not usually be needed after training\n",
    "  if os.path.exists(Patch_source):\n",
    "    shutil.rmtree(Patch_source)\n",
    "  if os.path.exists(Patch_target):\n",
    "    shutil.rmtree(Patch_target)\n",
    "  if os.path.exists(Patch_rejected):\n",
    "    shutil.rmtree(Patch_rejected)\n",
    "\n",
    "  os.mkdir(Patch_source)\n",
    "  os.mkdir(Patch_target)\n",
    "  os.mkdir(Patch_rejected) #This directory will contain the images that have too little signal.\n",
    "\n",
    "  patch_num = 0\n",
    "\n",
    "  for file in tqdm(os.listdir(Training_source)):\n",
    "\n",
    "    img = io.imread(os.path.join(Training_source, file))\n",
    "    mask = io.imread(os.path.join(Training_target, file),as_gray=True)\n",
    "\n",
    "    if DEBUG:\n",
    "      print(file)\n",
    "      print(img.dtype)\n",
    "\n",
    "    # Using view_as_windows with step size equal to the patch size to ensure there is no overlap\n",
    "    patches_img = view_as_windows(img, (patch_width, patch_height), (patch_width, patch_height))\n",
    "    patches_mask = view_as_windows(mask, (patch_width, patch_height), (patch_width, patch_height))\n",
    "\n",
    "    patches_img = patches_img.reshape(patches_img.shape[0]*patches_img.shape[1], patch_width,patch_height)\n",
    "    patches_mask = patches_mask.reshape(patches_mask.shape[0]*patches_mask.shape[1], patch_width,patch_height)\n",
    "\n",
    "    if DEBUG:\n",
    "      print(all_patches_img.shape)\n",
    "      print(all_patches_img.dtype)\n",
    "\n",
    "    for i in range(patches_img.shape[0]):\n",
    "      img_save_path = os.path.join(Patch_source,'patch_'+str(patch_num)+'.tif')\n",
    "      mask_save_path = os.path.join(Patch_target,'patch_'+str(patch_num)+'.tif')\n",
    "      patch_num += 1\n",
    "\n",
    "      # if the mask conatins at least 2% of its total number pixels as mask, then go ahead and save the images\n",
    "      pixel_threshold_array = sorted(patches_mask[i].flatten())\n",
    "      if pixel_threshold_array[int(round((len(pixel_threshold_array)-1)*(1-min_fraction)))]>0:\n",
    "        io.imsave(img_save_path, img_as_ubyte(normalizeMinMax(patches_img[i])))\n",
    "        io.imsave(mask_save_path, patches_mask[i])\n",
    "      else:\n",
    "        io.imsave(Patch_rejected+'/patch_'+str(patch_num)+'_image.tif', img_as_ubyte(normalizeMinMax(patches_img[i])))\n",
    "        io.imsave(Patch_rejected+'/patch_'+str(patch_num)+'_mask.tif', patches_mask[i])\n",
    "\n",
    "  return Patch_source, Patch_target\n",
    "\n",
    "\n",
    "def estimatePatchSize(data_path, max_width = 512, max_height = 512):\n",
    "\n",
    "  files = os.listdir(data_path)\n",
    "\n",
    "  # Get the size of the first image found in the folder and initialise the variables to that\n",
    "  n = 0\n",
    "  while os.path.isdir(os.path.join(data_path, files[n])):\n",
    "    n += 1\n",
    "  (height_min, width_min) = Image.open(os.path.join(data_path, files[n])).size\n",
    "\n",
    "  # Screen the size of all dataset to find the minimum image size\n",
    "  for file in files:\n",
    "    if not os.path.isdir(os.path.join(data_path, file)):\n",
    "      (height, width) = Image.open(os.path.join(data_path, file)).size\n",
    "      if width < width_min:\n",
    "        width_min = width\n",
    "      if height < height_min:\n",
    "        height_min = height\n",
    "\n",
    "  # Find the power of patches that will fit within the smallest dataset\n",
    "  width_min, height_min = (fittingPowerOfTwo(width_min), fittingPowerOfTwo(height_min))\n",
    "\n",
    "  # Clip values at maximum permissible values\n",
    "  if width_min > max_width:\n",
    "    width_min = max_width\n",
    "\n",
    "  if height_min > max_height:\n",
    "    height_min = max_height\n",
    "\n",
    "  return (width_min, height_min)\n",
    "\n",
    "def fittingPowerOfTwo(number):\n",
    "  n = 0\n",
    "  while 2**n <= number:\n",
    "    n += 1\n",
    "  return 2**(n-1)\n",
    "\n",
    "## TODO: create weighted CE for semantic labels\n",
    "def getClassWeights(Training_target_path):\n",
    "\n",
    "  Mask_dir_list = os.listdir(Training_target_path)\n",
    "  number_of_dataset = len(Mask_dir_list)\n",
    "\n",
    "  class_count = np.zeros(2, dtype=int)\n",
    "  for i in tqdm(range(number_of_dataset)):\n",
    "    mask = io.imread(os.path.join(Training_target_path, Mask_dir_list[i]))\n",
    "    mask = normalizeMinMax(mask)\n",
    "    class_count[0] += mask.shape[0]*mask.shape[1] - mask.sum()\n",
    "    class_count[1] += mask.sum()\n",
    "\n",
    "  n_samples = class_count.sum()\n",
    "  n_classes = 2\n",
    "\n",
    "  class_weights = n_samples / (n_classes * class_count)\n",
    "  return class_weights\n",
    "\n",
    "def weighted_binary_crossentropy(class_weights):\n",
    "\n",
    "    def _weighted_binary_crossentropy(y_true, y_pred):\n",
    "        binary_crossentropy = keras.binary_crossentropy(y_true, y_pred)\n",
    "        weight_vector = y_true * class_weights[1] + (1. - y_true) * class_weights[0]\n",
    "        weighted_binary_crossentropy = weight_vector * binary_crossentropy\n",
    "\n",
    "        return keras.mean(weighted_binary_crossentropy)\n",
    "\n",
    "    return _weighted_binary_crossentropy\n",
    "\n",
    "\n",
    "def save_augment(datagen,orig_img,dir_augmented_data=\"/content/augment\"):\n",
    "  \"\"\"\n",
    "  Saves a subset of the augmented data for visualisation, by default in /content.\n",
    "\n",
    "  This is adapted from: https://fairyonice.github.io/Learn-about-ImageDataGenerator.html\n",
    "\n",
    "  \"\"\"\n",
    "  try:\n",
    "    os.mkdir(dir_augmented_data)\n",
    "  except:\n",
    "        ## if the preview folder exists, then remove\n",
    "        ## the contents (pictures) in the folder\n",
    "    for item in os.listdir(dir_augmented_data):\n",
    "      os.remove(dir_augmented_data + \"/\" + item)\n",
    "\n",
    "    ## convert the original image to array\n",
    "  x = img_to_array(orig_img)\n",
    "    ## reshape (Sampke, Nrow, Ncol, 3) 3 = R, G or B\n",
    "    #print(x.shape)\n",
    "  x = x.reshape((1,) + x.shape)\n",
    "    #print(x.shape)\n",
    "    ## -------------------------- ##\n",
    "    ## randomly generate pictures\n",
    "    ## -------------------------- ##\n",
    "  i = 0\n",
    "    #We will just save 5 images,\n",
    "    #but this can be changed, but note the visualisation in 3. currently uses 5.\n",
    "  Nplot = 5\n",
    "  for batch in datagen.flow(x,batch_size=1,\n",
    "                            save_to_dir=dir_augmented_data,\n",
    "                            save_format='tif',\n",
    "                            seed=42):\n",
    "    i += 1\n",
    "    if i > Nplot - 1:\n",
    "      break\n",
    "\n",
    "# Generators\n",
    "def buildDoubleGenerator(image_datagen, mask_datagen, image_folder_path, mask_folder_path, subset, batch_size, target_size, validatio_split):\n",
    "  '''\n",
    "  Can generate image and mask at the same time use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n",
    "\n",
    "  datagen: ImageDataGenerator\n",
    "  subset: can take either 'training' or 'validation'\n",
    "  '''\n",
    "\n",
    "  # Build the dict for the ImageDataGenerator\n",
    "  # non_aug_args = dict(width_shift_range = 0,\n",
    "  #                     height_shift_range = 0,\n",
    "  #                     rotation_range = 0, #90\n",
    "  #                     zoom_range = 0,\n",
    "  #                     shear_range = 0,\n",
    "  #                     horizontal_flip = False,\n",
    "  #                     vertical_flip = False,\n",
    "  #                     fill_mode = 'reflect')\n",
    "  # default params of data generator is without augmentation\n",
    "  mask_load_gen = ImageDataGenerator(dtype='uint8', validation_split=validatio_split)\n",
    "  image_load_gen = ImageDataGenerator(dtype='float32', validation_split=validatio_split, preprocessing_function = normalizePercentile)\n",
    "\n",
    "  image_generator = image_load_gen.flow_from_directory(\n",
    "        os.path.dirname(image_folder_path),\n",
    "        classes = [os.path.basename(image_folder_path)],\n",
    "        class_mode = None,\n",
    "        color_mode = \"grayscale\",\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        subset = subset,\n",
    "        interpolation = \"bicubic\",\n",
    "        seed = 1)\n",
    "  mask_generator = mask_load_gen.flow_from_directory(\n",
    "        os.path.dirname(mask_folder_path),\n",
    "        classes = [os.path.basename(mask_folder_path)],\n",
    "        class_mode = None,\n",
    "        color_mode = \"grayscale\",\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        subset = subset,\n",
    "        interpolation = \"nearest\",\n",
    "        seed = 1)\n",
    "\n",
    "  this_generator = zip(image_generator, mask_generator)\n",
    "  for (img,mask) in this_generator:\n",
    "      if subset == 'training':\n",
    "          # Apply the data augmentation\n",
    "          # the same seed should provide always the same transformation and image loading\n",
    "          seed = np.random.randint(100000)\n",
    "          for batch_im in image_datagen.flow(img,batch_size=batch_size, seed=seed):\n",
    "              break\n",
    "          mask = mask.astype(np.float32)\n",
    "          labels = np.unique(mask)\n",
    "          if len(labels)>1:\n",
    "              batch_mask = np.zeros_like(mask, dtype='float32')\n",
    "              for l in range(0, len(labels)):\n",
    "                  aux = (mask==l).astype(np.float32)\n",
    "                  for batch_aux in mask_datagen.flow(aux,batch_size=batch_size, seed=seed):\n",
    "                      break\n",
    "                  batch_mask += l*(batch_aux>0).astype(np.float32)\n",
    "              index = np.where(batch_mask>l)\n",
    "              batch_mask[index]=l\n",
    "          else:\n",
    "              batch_mask = mask\n",
    "\n",
    "          yield (batch_im,batch_mask)\n",
    "\n",
    "      else:\n",
    "          yield (img,mask)\n",
    "\n",
    "\n",
    "def prepareGenerators(image_folder_path, mask_folder_path, datagen_parameters, batch_size = 4, target_size = (512, 512), validatio_split = 0.1):\n",
    "  image_datagen = ImageDataGenerator(**datagen_parameters, preprocessing_function = normalizePercentile)\n",
    "  mask_datagen = ImageDataGenerator(**datagen_parameters)\n",
    "\n",
    "  train_datagen = buildDoubleGenerator(image_datagen, mask_datagen, image_folder_path, mask_folder_path, 'training', batch_size, target_size, validatio_split)\n",
    "  validation_datagen = buildDoubleGenerator(image_datagen, mask_datagen, image_folder_path, mask_folder_path, 'validation', batch_size, target_size, validatio_split)\n",
    "\n",
    "  return (train_datagen, validation_datagen)\n",
    "\n",
    "\n",
    "# Normalization functions from Martin Weigert\n",
    "def normalizePercentile(x, pmin=1, pmax=99.8, axis=None, clip=False, eps=1e-20, dtype=np.float32):\n",
    "    \"\"\"This function is adapted from Martin Weigert\"\"\"\n",
    "    \"\"\"Percentile-based image normalization.\"\"\"\n",
    "\n",
    "    mi = np.percentile(x,pmin,axis=axis,keepdims=True)\n",
    "    ma = np.percentile(x,pmax,axis=axis,keepdims=True)\n",
    "    return normalize_mi_ma(x, mi, ma, clip=clip, eps=eps, dtype=dtype)\n",
    "\n",
    "\n",
    "def normalize_mi_ma(x, mi, ma, clip=False, eps=1e-20, dtype=np.float32):#dtype=np.float32\n",
    "    \"\"\"This function is adapted from Martin Weigert\"\"\"\n",
    "    if dtype is not None:\n",
    "        x   = x.astype(dtype,copy=False)\n",
    "        mi  = dtype(mi) if np.isscalar(mi) else mi.astype(dtype,copy=False)\n",
    "        ma  = dtype(ma) if np.isscalar(ma) else ma.astype(dtype,copy=False)\n",
    "        eps = dtype(eps)\n",
    "\n",
    "    try:\n",
    "        import numexpr\n",
    "        x = numexpr.evaluate(\"(x - mi) / ( ma - mi + eps )\")\n",
    "    except ImportError:\n",
    "        x =                   (x - mi) / ( ma - mi + eps )\n",
    "\n",
    "    if clip:\n",
    "        x = np.clip(x,0,1)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "# Simple normalization to min/max fir the Mask\n",
    "def normalizeMinMax(x, dtype=np.float32):\n",
    "  x = x.astype(dtype,copy=False)\n",
    "  x = (x - np.amin(x)) / (np.amax(x) - np.amin(x) + 1e-10)\n",
    "  return x\n",
    "\n",
    "\n",
    "# This is code outlines the architecture of U-net. The choice of pooling steps decides the depth of the network.\n",
    "def unet(pretrained_weights = None, input_size = (256,256,1), pooling_steps = 4, learning_rate = 1e-4, verbose=True, labels=2):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    # Downsampling steps\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "\n",
    "    if pooling_steps > 1:\n",
    "      pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "      conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "      conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "\n",
    "      if pooling_steps > 2:\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "        drop4 = Dropout(0.5)(conv4)\n",
    "\n",
    "        if pooling_steps > 3:\n",
    "          pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "          conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "          conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "          drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "          #Upsampling steps\n",
    "          up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "          merge6 = concatenate([drop4,up6], axis = 3)\n",
    "          conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "          conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    if pooling_steps > 2:\n",
    "      up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop4))\n",
    "      if pooling_steps > 3:\n",
    "        up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "      merge7 = concatenate([conv3,up7], axis = 3)\n",
    "      conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "      conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    if pooling_steps > 1:\n",
    "      up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv3))\n",
    "      if pooling_steps > 2:\n",
    "        up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "      merge8 = concatenate([conv2,up8], axis = 3)\n",
    "      conv8 = Conv2D(128, 3, activation= 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "      conv8 = Conv2D(128, 3, activation= 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    if pooling_steps == 1:\n",
    "      up9 = Conv2D(64, 2, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv2))\n",
    "    else:\n",
    "      up9 = Conv2D(64, 2, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8)) #activation = 'relu'\n",
    "\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(merge9) #activation = 'relu'\n",
    "    conv9 = Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv9) #activation = 'relu'\n",
    "    conv9 = Conv2D(labels, 3, padding = 'same', kernel_initializer = 'he_normal')(conv9) #activation = 'relu'\n",
    "    conv10 = Conv2D(labels, 1, activation = 'softmax')(conv9)\n",
    "\n",
    "    model = Model(inputs = inputs, outputs = conv10)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr = learning_rate), loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "    if verbose:\n",
    "      model.summary()\n",
    "\n",
    "    if(pretrained_weights):\n",
    "    \tmodel.load_weights(pretrained_weights);\n",
    "\n",
    "    return model\n",
    "\n",
    "# Custom callback showing sample prediction\n",
    "class SampleImageCallback(Callback):\n",
    "\n",
    "    def __init__(self, model, sample_data, model_path, save=False):\n",
    "        self.model = model\n",
    "        self.sample_data = sample_data\n",
    "        self.model_path = model_path\n",
    "        self.save = save\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "      if np.mod(epoch,5) == 0:\n",
    "            sample_predict = self.model.predict_on_batch(self.sample_data)\n",
    "\n",
    "            f=plt.figure(figsize=(16,8))\n",
    "            plt.subplot(1,labels+1,1)\n",
    "            plt.imshow(self.sample_data[0,:,:,0], cmap='gray')\n",
    "            plt.title('Sample source')\n",
    "            plt.axis('off');\n",
    "            for i in range(1, labels):\n",
    "              plt.subplot(1,labels+1,i+1)\n",
    "              plt.imshow(sample_predict[0,:,:,i], interpolation='nearest', cmap='magma')\n",
    "              plt.title('Predicted label {}'.format(i))\n",
    "              plt.axis('off');\n",
    "\n",
    "            plt.subplot(1,labels+1,labels+1)\n",
    "            plt.imshow(np.squeeze(np.argmax(sample_predict[0], axis=-1)), interpolation='nearest')\n",
    "            plt.title('Semantic segmentation')\n",
    "            plt.axis('off');\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            if self.save:\n",
    "                plt.savefig(self.model_path + '/epoch_' + str(epoch+1) + '.png')\n",
    "                random_choice = random.choice(os.listdir(Patch_source))\n",
    "\n",
    "def predict_as_tiles(Image_path, model):\n",
    "\n",
    "  # Read the data in and normalize\n",
    "  Image_raw = io.imread(Image_path, as_gray = True)\n",
    "  Image_raw = normalizePercentile(Image_raw)\n",
    "\n",
    "  # Get the patch size from the input layer of the model\n",
    "  patch_size = model.layers[0].output_shape[0][1:3]\n",
    "\n",
    "  # Pad the image with zeros if any of its dimensions is smaller than the patch size\n",
    "  if Image_raw.shape[0] < patch_size[0] or Image_raw.shape[1] < patch_size[1]:\n",
    "    Image = np.zeros((max(Image_raw.shape[0], patch_size[0]), max(Image_raw.shape[1], patch_size[1])))\n",
    "    Image[0:Image_raw.shape[0], 0: Image_raw.shape[1]] = Image_raw\n",
    "  else:\n",
    "    Image = Image_raw\n",
    "\n",
    "  # Calculate the number of patches in each dimension\n",
    "  n_patch_in_width = ceil(Image.shape[0]/patch_size[0])\n",
    "  n_patch_in_height = ceil(Image.shape[1]/patch_size[1])\n",
    "\n",
    "  prediction = np.zeros(Image.shape, dtype = 'uint8')\n",
    "\n",
    "  for x in range(n_patch_in_width):\n",
    "    for y in range(n_patch_in_height):\n",
    "      xi = patch_size[0]*x\n",
    "      yi = patch_size[1]*y\n",
    "\n",
    "      # If the patch exceeds the edge of the image shift it back\n",
    "      if xi+patch_size[0] >= Image.shape[0]:\n",
    "        xi = Image.shape[0]-patch_size[0]\n",
    "\n",
    "      if yi+patch_size[1] >= Image.shape[1]:\n",
    "        yi = Image.shape[1]-patch_size[1]\n",
    "\n",
    "      # Extract and reshape the patch\n",
    "      patch = Image[xi:xi+patch_size[0], yi:yi+patch_size[1]]\n",
    "      patch = np.reshape(patch,patch.shape+(1,))\n",
    "      patch = np.reshape(patch,(1,)+patch.shape)\n",
    "\n",
    "      # Get the prediction from the patch and paste it in the prediction in the right place\n",
    "      predicted_patch = model.predict(patch, batch_size = 1)\n",
    "      prediction[xi:xi+patch_size[0], yi:yi+patch_size[1]] = (np.argmax(np.squeeze(predicted_patch), axis = -1)).astype(np.uint8)\n",
    "\n",
    "\n",
    "  return prediction[0:Image_raw.shape[0], 0: Image_raw.shape[1]]\n",
    "\n",
    "\n",
    "def saveResult(save_path, nparray, source_dir_list, prefix=''):\n",
    "  for (filename, image) in zip(source_dir_list, nparray):\n",
    "      io.imsave(os.path.join(save_path, prefix+os.path.splitext(filename)[0]+'.tif'), image) # saving as unsigned 8-bit image\n",
    "\n",
    "\n",
    "def convert2Mask(image, threshold):\n",
    "  mask = img_as_ubyte(image, force_copy=True)\n",
    "  mask[mask > threshold] = 255\n",
    "  mask[mask <= threshold] = 0\n",
    "  return mask\n",
    "\n",
    "# -------------- Other definitions -----------\n",
    "W  = '\\033[0m'  # white (normal)\n",
    "R  = '\\033[31m' # red\n",
    "prediction_prefix = 'Predicted_'\n",
    "\n",
    "\n",
    "print('-------------------')\n",
    "print('U-Net and dependencies installed.')\n",
    "\n",
    "# Colors for the warning messages\n",
    "class bcolors:\n",
    "  WARNING = '\\033[31m'\n",
    "\n",
    "# Check if this is the latest version of the notebook\n",
    "\n",
    "All_notebook_versions = pd.read_csv(\"https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Latest_Notebook_versions.csv\", dtype=str)\n",
    "print('Notebook version: '+Notebook_version)\n",
    "Latest_Notebook_version = All_notebook_versions[All_notebook_versions[\"Notebook\"] == Network]['Version'].iloc[0]\n",
    "print('Latest notebook version: '+Latest_Notebook_version)\n",
    "if Notebook_version == Latest_Notebook_version:\n",
    "  print(\"This notebook is up-to-date.\")\n",
    "else:\n",
    "  print(bcolors.WARNING +\"A new version of this notebook has been released. We recommend that you download it at https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki\")\n",
    "\n",
    "\n",
    "def pdf_export(trained = False, augmentation = False, pretrained_model = False):\n",
    "  class MyFPDF(FPDF, HTMLMixin):\n",
    "    pass\n",
    "\n",
    "  pdf = MyFPDF()\n",
    "  pdf.add_page()\n",
    "  pdf.set_right_margin(-1)\n",
    "  pdf.set_font(\"Arial\", size = 11, style='B')\n",
    "\n",
    "  day = datetime.now()\n",
    "  datetime_str = str(day)[0:10]\n",
    "\n",
    "  Header = 'Training report for '+Network+' model ('+model_name+')\\nDate: '+datetime_str\n",
    "  pdf.multi_cell(180, 5, txt = Header, align = 'L')\n",
    "  pdf.ln(1)\n",
    "\n",
    "  # add another cell\n",
    "  if trained:\n",
    "    training_time = \"Training time: \"+str(hour)+ \"hour(s) \"+str(mins)+\"min(s) \"+str(round(sec))+\"sec(s)\"\n",
    "    pdf.cell(190, 5, txt = training_time, ln = 1, align='L')\n",
    "  pdf.ln(1)\n",
    "\n",
    "  Header_2 = 'Information for your materials and method:'\n",
    "  pdf.cell(190, 5, txt=Header_2, ln=1, align='L')\n",
    "  pdf.ln(1)\n",
    "\n",
    "  all_packages = ''\n",
    "  for requirement in freeze(local_only=True):\n",
    "    all_packages = all_packages+requirement+', '\n",
    "  #print(all_packages)\n",
    "\n",
    "  #Main Packages\n",
    "  main_packages = ''\n",
    "  version_numbers = []\n",
    "  for name in ['tensorflow','numpy','keras']:\n",
    "    find_name=all_packages.find(name)\n",
    "    main_packages = main_packages+all_packages[find_name:all_packages.find(',',find_name)]+', '\n",
    "    #Version numbers only here:\n",
    "    version_numbers.append(all_packages[find_name+len(name)+2:all_packages.find(',',find_name)])\n",
    "\n",
    "  cuda_version = subprocess.run('nvcc --version',stdout=subprocess.PIPE, shell=True)\n",
    "  cuda_version = cuda_version.stdout.decode('utf-8')\n",
    "  cuda_version = cuda_version[cuda_version.find(', V')+3:-1]\n",
    "  gpu_name = subprocess.run('nvidia-smi',stdout=subprocess.PIPE, shell=True)\n",
    "  gpu_name = gpu_name.stdout.decode('utf-8')\n",
    "  gpu_name = gpu_name[gpu_name.find('Tesla'):gpu_name.find('Tesla')+10]\n",
    "  #print(cuda_version[cuda_version.find(', V')+3:-1])\n",
    "  #print(gpu_name)\n",
    "  loss = str(model.loss)[str(model.loss).find('function')+len('function'):str(model.loss).find('.<')]\n",
    "  shape = io.imread(Training_source+'/'+os.listdir(Training_source)[1]).shape\n",
    "  dataset_size = len(os.listdir(Training_source))\n",
    "\n",
    "  text = 'The '+Network+' model was trained from scratch for '+str(number_of_epochs)+' epochs on '+str(number_of_training_dataset)+' paired image patches (image dimensions: '+str(shape)+', patch size: ('+str(patch_width)+','+str(patch_height)+')) with a batch size of '+str(batch_size)+' and a'+loss+' loss function,'+' using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). Key python packages used include tensorflow (v '+version_numbers[0]+'), keras (v '+version_numbers[2]+'), numpy (v '+version_numbers[1]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n",
    "\n",
    "  if pretrained_model:\n",
    "    text = 'The '+Network+' model was trained for '+str(number_of_epochs)+' epochs on '+str(number_of_training_dataset)+' paired image patches (image dimensions: '+str(shape)+', patch size: ('+str(patch_width)+','+str(patch_height)+')) with a batch size of '+str(batch_size)+'  and a'+loss+' loss function,'+' using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). The model was re-trained from a pretrained model. Key python packages used include tensorflow (v '+version_numbers[0]+'), keras (v '+version_numbers[2]+'), numpy (v '+version_numbers[1]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n",
    "\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font_size(10.)\n",
    "  pdf.multi_cell(180, 5, txt = text, align='L')\n",
    "  pdf.ln(1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 10, style = 'B')\n",
    "  pdf.cell(28, 5, txt='Augmentation: ', ln=1)\n",
    "  pdf.set_font('')\n",
    "  if augmentation:\n",
    "    aug_text = 'The dataset was augmented by'\n",
    "    if rotation_range != 0:\n",
    "      aug_text = aug_text+'\\n- rotation'\n",
    "    if horizontal_flip == True or vertical_flip == True:\n",
    "      aug_text = aug_text+'\\n- flipping'\n",
    "    if zoom_range != 0:\n",
    "      aug_text = aug_text+'\\n- random zoom magnification'\n",
    "    if horizontal_shift != 0 or vertical_shift != 0:\n",
    "      aug_text = aug_text+'\\n- shifting'\n",
    "    if shear_range != 0:\n",
    "      aug_text = aug_text+'\\n- image shearing'\n",
    "  else:\n",
    "    aug_text = 'No augmentation was used for training.'\n",
    "  pdf.multi_cell(190, 5, txt=aug_text, align='L')\n",
    "  pdf.ln(1)\n",
    "  pdf.set_font('Arial', size = 11, style = 'B')\n",
    "  pdf.cell(180, 5, txt = 'Parameters', align='L', ln=1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font_size(10.)\n",
    "  if Use_Default_Advanced_Parameters:\n",
    "    pdf.cell(200, 5, txt='Default Advanced Parameters were enabled')\n",
    "  pdf.cell(200, 5, txt='The following parameters were used for training:')\n",
    "  pdf.ln(1)\n",
    "  html = \"\"\"\n",
    "  <table width=40% style=\"margin-left:0px;\">\n",
    "    <tr>\n",
    "      <th width = 50% align=\"left\">Parameter</th>\n",
    "      <th width = 50% align=\"left\">Value</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>number_of_epochs</td>\n",
    "      <td width = 50%>{0}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>patch_size</td>\n",
    "      <td width = 50%>{1}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>batch_size</td>\n",
    "      <td width = 50%>{2}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>number_of_steps</td>\n",
    "      <td width = 50%>{3}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>percentage_validation</td>\n",
    "      <td width = 50%>{4}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>initial_learning_rate</td>\n",
    "      <td width = 50%>{5}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>pooling_steps</td>\n",
    "      <td width = 50%>{6}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>min_fraction</td>\n",
    "      <td width = 50%>{7}</td>\n",
    "  </table>\n",
    "  \"\"\".format(number_of_epochs, str(patch_width)+'x'+str(patch_height), batch_size, number_of_steps, percentage_validation, initial_learning_rate, pooling_steps, min_fraction)\n",
    "  pdf.write_html(html)\n",
    "\n",
    "  #pdf.multi_cell(190, 5, txt = text_2, align='L')\n",
    "  pdf.set_font(\"Arial\", size = 11, style='B')\n",
    "  pdf.ln(1)\n",
    "  pdf.cell(190, 5, txt = 'Training Dataset', align='L', ln=1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 10, style = 'B')\n",
    "  pdf.cell(29, 5, txt= 'Training_source:', align = 'L', ln=0)\n",
    "  pdf.set_font('')\n",
    "  pdf.multi_cell(170, 5, txt = Training_source, align = 'L')\n",
    "  pdf.ln(1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 10, style = 'B')\n",
    "  pdf.cell(28, 5, txt= 'Training_target:', align = 'L', ln=0)\n",
    "  pdf.set_font('')\n",
    "  pdf.multi_cell(170, 5, txt = Training_target, align = 'L')\n",
    "  pdf.ln(1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 10, style = 'B')\n",
    "  pdf.cell(21, 5, txt= 'Model Path:', align = 'L', ln=0)\n",
    "  pdf.set_font('')\n",
    "  pdf.multi_cell(170, 5, txt = model_path+'/'+model_name, align = 'L')\n",
    "  pdf.ln(1)\n",
    "  pdf.cell(60, 5, txt = 'Example Training pair', ln=1)\n",
    "  pdf.ln(1)\n",
    "  exp_size = io.imread('/content/TrainingDataExample_Unet2D.png').shape\n",
    "  pdf.image('/content/TrainingDataExample_Unet2D.png', x = 11, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n",
    "  pdf.ln(1)\n",
    "  ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"Democratising deep learning for microscopy with ZeroCostDL4Mic.\" Nature Communications (2021).'\n",
    "  pdf.multi_cell(190, 5, txt = ref_1, align='L')\n",
    "  pdf.ln(1)\n",
    "  ref_2 = '- Unet: Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015.'\n",
    "  pdf.multi_cell(190, 5, txt = ref_2, align='L')\n",
    "  # if Use_Data_augmentation:\n",
    "  #   ref_3 = '- Augmentor: Bloice, Marcus D., Christof Stocker, and Andreas Holzinger. \"Augmentor: an image augmentation library for machine learning.\" arXiv preprint arXiv:1708.04680 (2017).'\n",
    "  #   pdf.multi_cell(190, 5, txt = ref_3, align='L')\n",
    "  pdf.ln(3)\n",
    "  reminder = 'Important:\\nRemember to perform the quality control step on all newly trained models\\nPlease consider depositing your training dataset on Zenodo'\n",
    "  pdf.set_font('Arial', size = 11, style='B')\n",
    "  pdf.multi_cell(190, 5, txt=reminder, align='C')\n",
    "  pdf.ln(1)\n",
    "\n",
    "  pdf.output(model_path+'/'+model_name+'/'+model_name+'_training_report.pdf')\n",
    "\n",
    "  print('------------------------------')\n",
    "  print('PDF report exported in '+model_path+'/'+model_name+'/')\n",
    "\n",
    "def qc_pdf_export():\n",
    "  class MyFPDF(FPDF, HTMLMixin):\n",
    "    pass\n",
    "\n",
    "  pdf = MyFPDF()\n",
    "  pdf.add_page()\n",
    "  pdf.set_right_margin(-1)\n",
    "  pdf.set_font(\"Arial\", size = 11, style='B')\n",
    "\n",
    "  Network = 'Unet 2D'\n",
    "\n",
    "  day = datetime.now()\n",
    "  datetime_str = str(day)[0:10]\n",
    "\n",
    "  Header = 'Quality Control report for '+Network+' model ('+QC_model_name+')\\nDate: '+datetime_str\n",
    "  pdf.multi_cell(180, 5, txt = Header, align = 'L')\n",
    "  pdf.ln(1)\n",
    "\n",
    "  all_packages = ''\n",
    "  for requirement in freeze(local_only=True):\n",
    "    all_packages = all_packages+requirement+', '\n",
    "\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 11, style = 'B')\n",
    "  pdf.ln(2)\n",
    "  pdf.cell(190, 5, txt = 'Loss curves', ln=1, align='L')\n",
    "  pdf.ln(1)\n",
    "  exp_size = io.imread(full_QC_model_path+'/Quality Control/QC_example_data.png').shape\n",
    "  if os.path.exists(full_QC_model_path+'/Quality Control/lossCurvePlots.png'):\n",
    "    pdf.image(full_QC_model_path+'/Quality Control/lossCurvePlots.png', x = 11, y = None, w = round(exp_size[1]/12), h = round(exp_size[0]/3))\n",
    "  else:\n",
    "    pdf.set_font('')\n",
    "    pdf.set_font('Arial', size=10)\n",
    "    pdf.multi_cell(190, 5, txt='If you would like to see the evolution of the loss function during training please play the first cell of the QC section in the notebook.',align='L')\n",
    "  pdf.ln(2)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 10, style = 'B')\n",
    "  pdf.ln(3)\n",
    "  pdf.cell(80, 5, txt = 'Example Quality Control Visualisation', ln=1)\n",
    "  pdf.ln(1)\n",
    "  exp_size = io.imread(full_QC_model_path+'/Quality Control/QC_example_data.png').shape\n",
    "  pdf.image(full_QC_model_path+'/Quality Control/QC_example_data.png', x = 16, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n",
    "  pdf.ln(1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 11, style = 'B')\n",
    "  pdf.ln(1)\n",
    "  pdf.cell(180, 5, txt = 'Quality Control Metrics', align='L', ln=1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font_size(10.)\n",
    "\n",
    "  pdf.ln(1)\n",
    "  html = \"\"\"\n",
    "  <body>\n",
    "  <font size=\"10\" face=\"Courier\" >\n",
    "  <table width=60% style=\"margin-left:0px;\">\"\"\"\n",
    "  with open(full_QC_model_path+'/Quality Control/QC_metrics_'+QC_model_name+'.csv', 'r') as csvfile:\n",
    "    metrics = csv.reader(csvfile)\n",
    "    header = next(metrics)\n",
    "    image = header[0]\n",
    "    IoU = header[-1]\n",
    "    header = \"\"\"\n",
    "    <tr>\n",
    "    <th width = 33% align=\"center\">{0}</th>\n",
    "    <th width = 33% align=\"center\">{1}</th>\n",
    "    </tr>\"\"\".format(image,IoU)\n",
    "    html = html+header\n",
    "    i=0\n",
    "    for row in metrics:\n",
    "      i+=1\n",
    "      image = row[0]\n",
    "      IoU = row[-1]\n",
    "      cells = \"\"\"\n",
    "        <tr>\n",
    "          <td width = 33% align=\"center\">{0}</td>\n",
    "          <td width = 33% align=\"center\">{1}</td>\n",
    "        </tr>\"\"\".format(image,str(round(float(IoU),3)))\n",
    "      html = html+cells\n",
    "    html = html+\"\"\"</body></table>\"\"\"\n",
    "\n",
    "  pdf.write_html(html)\n",
    "\n",
    "  pdf.ln(1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font_size(10.)\n",
    "  ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"Democratising deep learning for microscopy with ZeroCostDL4Mic.\" Nature Communications (2021).'\n",
    "  pdf.multi_cell(190, 5, txt = ref_1, align='L')\n",
    "  pdf.ln(1)\n",
    "  ref_2 = '- Unet: Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015.'\n",
    "  pdf.multi_cell(190, 5, txt = ref_2, align='L')\n",
    "  pdf.ln(3)\n",
    "  reminder = 'To find the parameters and other information about how this model was trained, go to the training_report.pdf of this model which should be in the folder of the same name.'\n",
    "\n",
    "  pdf.set_font('Arial', size = 11, style='B')\n",
    "  pdf.multi_cell(190, 5, txt=reminder, align='C')\n",
    "  pdf.ln(1)\n",
    "\n",
    "  pdf.output(full_QC_model_path+'/Quality Control/'+QC_model_name+'_QC_report.pdf')\n",
    "\n",
    "  print('------------------------------')\n",
    "  print('QC PDF report exported as '+full_QC_model_path+'/Quality Control/'+QC_model_name+'_QC_report.pdf')\n",
    "\n",
    "# Build requirements file for local run\n",
    "after = [str(m) for m in sys.modules]\n",
    "build_requirements_file(before, after)\n",
    "!pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8wuQGjoq6eN",
    "tags": []
   },
   "source": [
    "**Generate prediction(s) from unseen dataset**\n",
    "---\n",
    "\n",
    "<font size = 4>The current trained model can now be used to process images. If you want to use an older model, untick the **Use_the_current_trained_model** box and enter the name and path of the model to use. Predicted output images are saved in your **Result_folder** folder.\n",
    "\n",
    "<font size = 4>**`Data_folder`:** This folder should contain the images that you want to use your trained network on for processing.\n",
    "\n",
    "<font size = 4>**`Result_folder`:** This folder will contain the predicted output images.\n",
    "\n",
    "<font size = 4> Once the predictions are complete the cell will display a random example prediction beside the input image and the calculated mask for visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0fea460fec1d4f0fa920e5b911e6766c",
      "2110815380a74b61b48465c58f039657",
      "839a0e6dd2fa43c3b3449f005954fd8d",
      "39ddb6dc102349dab978f327fce7d2e5",
      "26be94d5fcc748d89c0cc29ba5c84586",
      "e377a0d14e74455d8cd8682284fd3ab0",
      "93a261338db0450e84500da4fd07eb00",
      "cda43fd8d4f64029823c1e173f79cc3e",
      "431a9ffe0be64e74aac0b8ac4fbf05fb",
      "f97ae49f1d524ba9b859289b8546927d",
      "9d0f27808e9140d7a9612f2fb70e6772"
     ]
    },
    "id": "y2TD5p7MZrEb",
    "outputId": "82e4c90c-d8ea-426e-92ef-8bc88d5e10ea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------- Initial user input ------------\n",
    "#@markdown ###Provide the path to your dataset and to the folder where the predicted masks will be saved (Result folder), then play the cell to predict the output on your unseen images and store it.\n",
    "Data_folder = 'E:/AAV para enfermedades renales/LSFM combined images/Full images/MacroSPIM3/R5LEC-Path_0.5_Lectine' #@param {type:\"string\"}\n",
    "\n",
    "Results_folder = 'E:/AAV para enfermedades renales/LSFM combined images/CystSegmentation - Itsaso/InitialSegMasks/MacroSPIM3/'+\\\n",
    "            'R5LEC-Path_0.5_Lectine' #@param {type:\"string\"}\n",
    "\n",
    "if not os.path.exists(Results_folder):\n",
    "  os.makedirs(Results_folder)\n",
    "\n",
    "#@markdown ###Do you want to use the current trained model?\n",
    "Use_the_current_trained_model = False #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ###If not, please provide the path to the model folder:\n",
    "\n",
    "Prediction_model_folder = \"E:/AAV para enfermedades renales/LSFM combined images/CystSegmentation - Itsaso/Cyst_PretrainedModel\" #@param {type:\"string\"}\n",
    "\n",
    "#Here we find the loaded model name and parent path\n",
    "Prediction_model_name = os.path.basename(Prediction_model_folder)\n",
    "Prediction_model_path = os.path.dirname(Prediction_model_folder)\n",
    "\n",
    "# ------------- Failsafes ------------\n",
    "if (Use_the_current_trained_model):\n",
    "  print(\"Using current trained network\")\n",
    "  Prediction_model_name = model_name\n",
    "  Prediction_model_path = model_path\n",
    "\n",
    "full_Prediction_model_path = os.path.join(Prediction_model_path, Prediction_model_name)\n",
    "if os.path.exists(full_Prediction_model_path):\n",
    "  print(\"The \"+Prediction_model_name+\" network will be used.\")\n",
    "else:\n",
    "  print(R+'!! WARNING: The chosen model does not exist !!'+W)\n",
    "  print('Please make sure you provide a valid model path and model name before proceeding further.')\n",
    "\n",
    "# ------------- Prepare the model and run predictions ------------\n",
    "\n",
    "# Load the model and prepare generator\n",
    "\n",
    "\n",
    "\n",
    "unet = load_model(os.path.join(Prediction_model_path, Prediction_model_name, 'weights_best.h5'), compile=False)\n",
    "\n",
    "unet.compile()\n",
    "\n",
    "\n",
    "Input_size = unet.layers[0].output_shape[0][1:3]\n",
    "print('Model input size: '+str(Input_size[0])+'x'+str(Input_size[1]))\n",
    "\n",
    "# Create a list of sources\n",
    "source_dir_list = os.listdir(Data_folder)\n",
    "number_of_dataset = len(source_dir_list)\n",
    "print('Number of dataset found in the folder: '+str(number_of_dataset))\n",
    "\n",
    "predictions = []\n",
    "prefix=''\n",
    "for i in tqdm(range(number_of_dataset)):\n",
    "  \n",
    "  mask = predict_as_tiles(os.path.join(Data_folder, source_dir_list[i]), unet)\n",
    "\n",
    "  io.imsave(os.path.join(Results_folder, prefix+os.path.splitext(source_dir_list[i])[0]+'.tif'), mask) # saving as unsigned 8-bit image\n",
    "\n",
    "  #predictions.append(predict_as_tiles(os.path.join(Data_folder, source_dir_list[i]), unet))\n",
    "\n",
    "  #saveResult(Results_folder, predictions, source_dir_list, prefix=prediction_prefix)\n",
    "  # predictions.append(prediction(os.path.join(Data_folder, source_dir_list[i]), os.path.join(Prediction_model_path, Prediction_model_name)))\n",
    "\n",
    "\n",
    "# Save the results in the folder along with the masks according to the set threshold\n",
    "saveResult(Results_folder, predictions, source_dir_list, prefix=prediction_prefix)\n",
    "\n",
    "\n",
    "# ------------- For display ------------\n",
    "print('--------------------------------------------------------------')\n",
    "\n",
    "\n",
    "def show_prediction_mask(file=os.listdir(Data_folder)):\n",
    "\n",
    "  plt.figure(figsize=(10,6))\n",
    "  # Wide-field\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.axis('off')\n",
    "  img_Source = plt.imread(os.path.join(Data_folder, file))\n",
    "  plt.imshow(img_Source, cmap='gray')\n",
    "  plt.title('Source image',fontsize=15)\n",
    "  # Prediction\n",
    "  plt.subplot(1,2,2)\n",
    "  plt.axis('off')\n",
    "  img_Prediction = plt.imread(os.path.join(Results_folder, prediction_prefix+file))\n",
    "  plt.imshow(img_Prediction, cmap='gray')\n",
    "  plt.title('Prediction',fontsize=15)\n",
    "\n",
    "interact(show_prediction_mask)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "AdN8B91xZO0x",
    "n4yWFoJNnoin",
    "sNIVx8_CLolt",
    "-tJeeJjLnRkP"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0fea460fec1d4f0fa920e5b911e6766c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2110815380a74b61b48465c58f039657",
       "IPY_MODEL_839a0e6dd2fa43c3b3449f005954fd8d",
       "IPY_MODEL_39ddb6dc102349dab978f327fce7d2e5"
      ],
      "layout": "IPY_MODEL_26be94d5fcc748d89c0cc29ba5c84586"
     }
    },
    "2110815380a74b61b48465c58f039657": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e377a0d14e74455d8cd8682284fd3ab0",
      "placeholder": "​",
      "style": "IPY_MODEL_93a261338db0450e84500da4fd07eb00",
      "value": " 22%"
     }
    },
    "26be94d5fcc748d89c0cc29ba5c84586": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39ddb6dc102349dab978f327fce7d2e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f97ae49f1d524ba9b859289b8546927d",
      "placeholder": "​",
      "style": "IPY_MODEL_9d0f27808e9140d7a9612f2fb70e6772",
      "value": " 529/2367 [1:01:33&lt;7:41:58, 15.08s/it]"
     }
    },
    "431a9ffe0be64e74aac0b8ac4fbf05fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "839a0e6dd2fa43c3b3449f005954fd8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cda43fd8d4f64029823c1e173f79cc3e",
      "max": 2367,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_431a9ffe0be64e74aac0b8ac4fbf05fb",
      "value": 529
     }
    },
    "93a261338db0450e84500da4fd07eb00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d0f27808e9140d7a9612f2fb70e6772": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cda43fd8d4f64029823c1e173f79cc3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e377a0d14e74455d8cd8682284fd3ab0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f97ae49f1d524ba9b859289b8546927d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
