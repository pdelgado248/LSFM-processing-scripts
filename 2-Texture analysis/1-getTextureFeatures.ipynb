{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define functions to extract features from images and test a method\n",
    "#Always run this first cell for all the imports and global variable definitions\n",
    "#To be run in the workenvAutoencoder2 environment\n",
    "\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import pywt\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.transform import radon\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.ndimage import generic_filter\n",
    "from scipy.stats import kurtosis, skew\n",
    "import nibabel as nib\n",
    "import SimpleITK as sitk\n",
    "import mahotas as mh\n",
    "import tifffile as tif\n",
    "import xgboost as xgb\n",
    "\n",
    "from radiomics import featureextractor\n",
    "\n",
    "MAIN_DIR = 'E:/AAV para enfermedades renales/LSFM combined images/TextureAnalysis/Tiles3D/CenteredInGlom-prepared'\n",
    "\n",
    "\n",
    "def normalize_image(image):\n",
    "    # Normalize the image to the range [0, 1]\n",
    "    image = image - np.min(image)\n",
    "    image = image / np.max(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "#Define several functions to extract features from an image\n",
    "#-----------------------------------------------------------------------------------------\n",
    "\n",
    "#General Features\n",
    "def calculate_statistical_features(image):\n",
    "    mean_val = np.mean(image)\n",
    "    variance_val = np.var(image)\n",
    "    skewness_val = skew(image,axis=None)\n",
    "    kurtosis_val = kurtosis(image,axis=None)\n",
    "    #hist, _ = np.histogram(image, bins=256, range=(0, 255))\n",
    "    #hist = hist / np.sum(hist)\n",
    "    #entropy_val = -np.sum(hist * np.log2(hist + np.finfo(float).eps))\n",
    "    return mean_val, variance_val, skewness_val, kurtosis_val\n",
    "    \n",
    "#Wavelet features in 3D\n",
    "def calculate_wavelet_features3D(image):\n",
    "    # Perform 3D discrete wavelet transform\n",
    "    coeffs = pywt.dwtn(image, 'db1')\n",
    "    \n",
    "    # Extract approximation and detail coefficients\n",
    "    cA = coeffs['aaa']\n",
    "    cH = coeffs['daa']\n",
    "    cV = coeffs['ada']\n",
    "    cD = coeffs['aad']\n",
    "    \n",
    "    # Compute the mean of the coefficients\n",
    "    approx_coeff_mean = np.mean(cA)\n",
    "    horiz_coeff_mean = np.mean(cH)\n",
    "    vert_coeff_mean = np.mean(cV)\n",
    "    depth_coeff_mean = np.mean(cD)\n",
    "\n",
    "    return approx_coeff_mean, horiz_coeff_mean, vert_coeff_mean, depth_coeff_mean\n",
    "\n",
    "    \n",
    "#Fourier features in 3D\n",
    "def calculate_fourier_features_3d(image):\n",
    "    # Perform 3D Fast Fourier Transform\n",
    "    fft_image = np.fft.fftn(image)\n",
    "    fft_magnitude = np.abs(fft_image)\n",
    "\n",
    "    # Find the dominant frequency\n",
    "    dominant_frequency_magnitude = np.max(fft_magnitude)\n",
    "    \n",
    "    # Calculate the total energy\n",
    "    total_energy = np.sum(fft_magnitude ** 2)\n",
    "\n",
    "    return dominant_frequency_magnitude, total_energy\n",
    "    \n",
    "#Local Binary Patterns (LBP) uniformity in 3D\n",
    "def lbp_code_3d(neighborhood):\n",
    "    center = neighborhood[len(neighborhood) // 2]\n",
    "    binary_pattern = (neighborhood >= center).astype(int)\n",
    "    binary_pattern = np.delete(binary_pattern, len(binary_pattern) // 2)  # Remove the center pixel\n",
    "    lbp_value = np.dot(binary_pattern, 1 << np.arange(binary_pattern.size)[::-1])\n",
    "    return lbp_value\n",
    "\n",
    "def calculate_lbp_uniformity_3d(image, radius=1):\n",
    "    # Define the neighborhood shape for 3D LBP\n",
    "    neighborhood_shape = (2 * radius + 1, 2 * radius + 1, 2 * radius + 1)\n",
    "    \n",
    "    # Apply the 3D LBP calculation\n",
    "    lbp_image = generic_filter(image, lbp_code_3d, size=neighborhood_shape, mode='constant', cval=0)\n",
    "    \n",
    "    # Compute the histogram of LBP values\n",
    "    lbp_hist, _ = np.histogram(lbp_image.ravel(), bins=np.arange(0, 2**(neighborhood_shape[0]**3 - 1) + 1), range=(0, 2**(neighborhood_shape[0]**3 - 1)))\n",
    "    lbp_hist = lbp_hist / np.sum(lbp_hist)  # Normalize histogram\n",
    "    \n",
    "    # Calculate uniformity measure\n",
    "    lbp_uniformity = np.sum(lbp_hist**2)\n",
    "\n",
    "    return lbp_uniformity\n",
    "\n",
    "\n",
    "def calculate_porosity(image):\n",
    "    threshold = np.mean(image)  # Use mean intensity as a threshold\n",
    "    hole_area = np.sum(image < threshold)  # Count low-intensity pixels (holes)\n",
    "    total_area = image.size  # Total number of pixels\n",
    "    porosity = hole_area / total_area\n",
    "    return porosity\n",
    "#-----------------------------------------------------------------------------------------\n",
    "\n",
    "#Define a function to extract all features from a set of images and produce a numpy array\n",
    "def extract_features(MAIN_DIR,setName):\n",
    "    # Define GLCM parameters\n",
    "    featureNames = 'mean_val', 'variance_val', 'skewness_val', 'kurtosis_val',\\\n",
    "    'approx_coeff_mean', 'horiz_coeff_mean', 'vert_coeff_mean', 'depth_coeff_mean',\\\n",
    "    'dominant_frequency_magnitude', 'total_energy',\\\n",
    "    'lbp_uniformity',\\\n",
    "    'porosity'\n",
    "    \n",
    "    initDir = os.getcwd()\n",
    "\n",
    "    os.chdir(MAIN_DIR)\n",
    "\n",
    "    #Entry 0 to store the Healthy features, entry 1 to store the Pathological features\n",
    "    data = [[], []]\n",
    "\n",
    "    print(f'Processing images from the {setName} set:')\n",
    "\n",
    "    for stateName in os.listdir(f'{setName}'):\n",
    "        #For the Healthy and Pathological folders\n",
    "        if stateName == 'Healthy':\n",
    "            stateNum = 0\n",
    "            \n",
    "        elif stateName == 'Pathological':\n",
    "            stateNum = 1\n",
    "            \n",
    "        print(f'{stateName} images')\n",
    "\n",
    "        for imName in tqdm(os.listdir(f'{setName}/{stateName}')):\n",
    "                \n",
    "            # Load and normalize the image\n",
    "            image = tif.imread(f'{setName}/{stateName}/{imName}')\n",
    "            image = normalize_image(image)\n",
    "\n",
    "            # Statistical features\n",
    "            mean_val, variance_val, skewness_val, kurtosis_val = calculate_statistical_features(image)         \n",
    "            \n",
    "            # Wavelet features\n",
    "            approx_coeff_mean, horiz_coeff_mean, vert_coeff_mean, depth_coeff_mean = calculate_wavelet_features3D(image)\n",
    "\n",
    "            # Fourier features\n",
    "            dominant_frequency_magnitude, total_energy = calculate_fourier_features_3d(image)\n",
    "\n",
    "            # Average diameter of circles with radon transform\n",
    "            #average_diameter_radon,average_radial_intensity_radon = measure_average_diameter_radon(image)\n",
    "\n",
    "            #LBP uniformity\n",
    "            lbp_uniformity = calculate_lbp_uniformity_3d(image)\n",
    "\n",
    "            #Porosity\n",
    "            porosity = calculate_porosity(image)\n",
    "\n",
    "            features = np.array([mean_val, variance_val, skewness_val, kurtosis_val,  \n",
    "                                 approx_coeff_mean, horiz_coeff_mean, vert_coeff_mean, depth_coeff_mean,\n",
    "                                 dominant_frequency_magnitude, total_energy, \n",
    "                                 lbp_uniformity,\n",
    "                                 porosity])\n",
    "            \n",
    "            #Save the features in the corresponding list\n",
    "            data[stateNum].append(features)\n",
    "\n",
    "    healthyFeatures = np.vstack(data[0])\n",
    "    pathologicalFeatures = np.vstack(data[1])\n",
    "\n",
    "    healthyLabels = np.zeros(healthyFeatures.shape[0],dtype=int)\n",
    "    pathologicalLabels = np.ones(pathologicalFeatures.shape[0],dtype=int)\n",
    "\n",
    "    allFeatures = np.vstack([healthyFeatures, pathologicalFeatures])\n",
    "    allLabels = np.hstack([healthyLabels, pathologicalLabels])\n",
    "\n",
    "    os.chdir(initDir)\n",
    "    return allFeatures, allLabels, featureNames\n",
    "\n",
    "def get_test_results(method,testData,testLabels,dataTuple = False):\n",
    "    #Test a method and print the results\n",
    "\n",
    "    if dataTuple is True:\n",
    "        #The dataformat is a tuple with the features and the labels\n",
    "        testData = xgb.DMatrix(testData, label=testLabels)\n",
    "\n",
    "    #Positives will be the Pathological images\n",
    "    truePos = 0\n",
    "    trueNeg = 0\n",
    "    falsePos = 0\n",
    "    falseNeg = 0\n",
    "\n",
    "    predictions = method.predict(testData)\n",
    "\n",
    "    #If the predictions are not a binary vector\n",
    "    if not np.all(np.isin(predictions, [0, 1])):\n",
    "        #Threshold them to make them binary\n",
    "        predictions = np.where(predictions >= 0.5, 1, 0)\n",
    "\n",
    "    print('predictions')\n",
    "    print(predictions)\n",
    "\n",
    "    print('ground truth labels')\n",
    "    print(testLabels)\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == testLabels[i] and testLabels[i] == 1:\n",
    "            truePos += 1\n",
    "        elif predictions[i] == testLabels[i] and testLabels[i] == 0:\n",
    "            trueNeg += 1\n",
    "        elif predictions[i] != testLabels[i] and testLabels[i] == 0:\n",
    "            falsePos += 1\n",
    "        elif predictions[i] != testLabels[i] and testLabels[i] == 1:\n",
    "            falseNeg += 1\n",
    "        #print(f'Predicted: {predictions[i]}, True: {test_labels[i]}')\n",
    "\n",
    "    print(f'Total predictions: {len(predictions)}')\n",
    "    print(f'True positives: {truePos}')\n",
    "    print(f'True negatives: {trueNeg}')\n",
    "    print(f'False positives: {falsePos}')\n",
    "    print(f'False negatives: {falseNeg}')\n",
    "    print(f'Accuracy: {(truePos+trueNeg)/len(predictions)}')\n",
    "    print(f'Sensitivity: {truePos/(truePos+falseNeg)}')\n",
    "    print(f'Specificity: {trueNeg/(trueNeg+falsePos)}')\n",
    "    print(f'f1-score: {2*truePos/(2*truePos+falsePos+falseNeg)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract features from the images and save them in text files\n",
    "\n",
    "testData,testLabels,featureNames = extract_features(MAIN_DIR,'test')\n",
    "header = ','.join(featureNames)\n",
    "np.savetxt('testData.txt', testData, delimiter=',', header=header, comments='')\n",
    "np.savetxt('testLabels.txt', testLabels, delimiter=',', header='Label', comments='')\n",
    "\n",
    "valData,valLabels,featureNames = extract_features(MAIN_DIR,'validation')\n",
    "header = ','.join(featureNames)\n",
    "np.savetxt('valData.txt', valData, delimiter=',', header=header, comments='')\n",
    "np.savetxt('valLabels.txt', valLabels, delimiter=',', header='Label', comments='', fmt='%d')\n",
    "\n",
    "trainData,trainLabels,featureNames = extract_features(MAIN_DIR,'train')\n",
    "header = ','.join(featureNames)\n",
    "np.savetxt('trainData.txt', trainData, delimiter=',', header=header, comments='')\n",
    "np.savetxt('trainLabels.txt', trainLabels, delimiter=',', header='Label', comments='', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data from the files to later apply a classifier\n",
    "\n",
    "\n",
    "#First read the feature names\n",
    "featureNames = np.genfromtxt('testData.txt', delimiter=',', max_rows=1, dtype=str)\n",
    "\n",
    "#Then load the data\n",
    "testData = np.loadtxt('testData.txt', delimiter=',', skiprows=1)\n",
    "testLabels = np.loadtxt('testLabels.txt', delimiter=',', skiprows=1)\n",
    "\n",
    "valData = np.loadtxt('valData.txt', delimiter=',', skiprows=1)\n",
    "valLabels = np.loadtxt('valLabels.txt', delimiter=',', skiprows=1)\n",
    "\n",
    "trainData = np.loadtxt('trainData.txt', delimiter=',', skiprows=1)\n",
    "trainLabels = np.loadtxt('trainLabels.txt', delimiter=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an XGBoost classifier and test it (with XGBClassifier), after performing hyperparameter grid search\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'reg_alpha': [0, 0.01, 0.1],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "# Combine train and validation data\n",
    "combinedData = np.concatenate((trainData, valData), axis=0)\n",
    "combinedLabels = np.concatenate((trainLabels, valLabels), axis=0)\n",
    "\n",
    "# Create a validation fold index\n",
    "test_fold = np.concatenate((\n",
    "    np.full(trainData.shape[0], -1),  # -1 for training set\n",
    "    np.zeros(valData.shape[0])        # 0 for validation set\n",
    "))\n",
    "\n",
    "# Create PredefinedSplit\n",
    "ps = PredefinedSplit(test_fold)\n",
    "\n",
    "# Initialize the model\n",
    "model = XGBClassifier(\n",
    "    objective='binary:logistic', \n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,  \n",
    "    tree_method='gpu_hist'  \n",
    ")\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=ps,  # Use PredefinedSplit\n",
    "    verbose=2,  # Set verbose to 2 for detailed progress information\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(combinedData, combinedLabels)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "# Save the best parameters to a text file\n",
    "with open('best_params.txt', 'w') as file:\n",
    "    json.dump(best_params, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the best weights saved from a hyperparameter grid search and fit a classification model\n",
    "\n",
    "import json\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Read the best parameters from the text file\n",
    "with open('best_params.txt', 'r') as file:\n",
    "    best_params = json.load(file)\n",
    "\n",
    "# Initialize the model with the best parameters\n",
    "model2 = XGBClassifier(\n",
    "    objective='binary:logistic', \n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,  # For compatibility\n",
    "    tree_method='gpu_hist',  # Use GPU for training if available\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "# Fit the model using the training data\n",
    "model2.fit(trainData, trainLabels, eval_set=[(valData, valLabels)], verbose=False)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_prob = model2.predict_proba(testData)[:, 1]  # Probabilities\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)  # Convert to binary predictions\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(testLabels, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()  # Extract TN, FP, FN, TP\n",
    "\n",
    "# Print results\n",
    "print(\"\\nClassification Report:\\n\", classification_report(testLabels, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "\n",
    "# Get feature importances\n",
    "importances = model2.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': featureNames,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the feature importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(feature_importance_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workenvAutoencoder2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
